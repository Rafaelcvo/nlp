{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings em um Dataset Aberto\n",
    "\n",
    "## Terceiro Exercício Prático\n",
    "\n",
    "\n",
    "Neste TP você deve explorar e criar os embeddings utilizando uma base de dados de sua preferência. Uma sugestão para encontrar bases interessantes seria a partir das competições do [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "Para essa prática, solicito que o aluno prepare a base de dados e gere os embeddings utilizando obrigatoriamente o algoritmo Word2Vec. Dependendo do contexto da base, você pode utilizar o Doc2Vec em vez do Word2Vec ou ambos. \n",
    "\n",
    "As 3 etapas descritas abaixo devem ser seguidas obrigatoriamente:\n",
    "\n",
    "1. Preparação da base de dados assim como visto na prática anterior.\n",
    "2. Execução do Modelo Word2Vec usando o Gensim, ou outra implementação similar.\n",
    "3. Teste do seu embedding assim como foi realizado na [demo](https://github.com/gesteves91/nlp/blob/master/notebooks/06-word2vec.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o trabalho foi capturado os tweets sobre o ex-presidente Lula no dia em que recebeu autorização para deixar a cadeia. Especificamos o dia 08/11/2019 (data em que ele saiu), restringindo a localização na região de Belo Horizonte, num raio de 10.000 km.\n",
    "\n",
    "Documentação:<br> \n",
    "https://tweepy.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import nltk\n",
    "import demoji\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura das chaves \n",
    "with open('twitter-tokens.txt', 'r') as tfile:\n",
    "    consumer_key = tfile.readline().strip('\\n')\n",
    "    consumer_secret = tfile.readline().strip('\\n')\n",
    "    access_token = tfile.readline().strip('\\n')\n",
    "    access_token_secret = tfile.readline().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variaveis para fazer o login\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query passando a busca e excluindo os retweets.\n",
    "query_search = 'Lula' + '-filter:retweets'\n",
    "\n",
    "#Gerando o cursor de busca.\n",
    "cursor_tweets = tweepy.Cursor(api.search, q=query_search, tweet_mode='extended',lang=\"pt\", \n",
    "                              since= \"2019-11-07\", until= \"2019-11-09\",\n",
    "                              geocode='-19.9026615,-44.1041363,10000km').items(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = []\n",
    "for tweet in cursor_tweets:\n",
    "    tw.append([tweet.created_at, tweet.full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tw, columns=['Data','Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo a data\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "df['Data'] = df['Data'].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando em um arquivo csv.\n",
    "df.to_csv('bh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bh = pd.read_csv('bh3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6750, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Data</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>08-11-2019</td>\n",
       "      <td>Lula virou comediante na cadeia e saiu fazendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>08-11-2019</td>\n",
       "      <td>Hoj o Lula transa com o super pênis dele https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>08-11-2019</td>\n",
       "      <td>@ana_claudiinha até o Lula tá beijando e você ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>08-11-2019</td>\n",
       "      <td>@ggreenwald Bravo! Bravo! Você tem grande impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>08-11-2019</td>\n",
       "      <td>Vou me ausentar desse site, quando pararem de ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Data                                              Tweet\n",
       "0           0  08-11-2019  Lula virou comediante na cadeia e saiu fazendo...\n",
       "1           1  08-11-2019  Hoj o Lula transa com o super pênis dele https...\n",
       "2           2  08-11-2019  @ana_claudiinha até o Lula tá beijando e você ...\n",
       "3           3  08-11-2019  @ggreenwald Bravo! Bravo! Você tem grande impo...\n",
       "4           4  08-11-2019  Vou me ausentar desse site, quando pararem de ..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_bh[['Data', 'Tweet']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removendo stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remov_stopwords(text):\n",
    "    text = text.lower()\n",
    "    palavras = [i for i in text.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df.apply(lambda row: remov_stopwords(row['Tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removendo links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df.apply(lambda x: re.sub(r\"http\\S+\", \"\", x['Tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substituindo emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(emoji):\n",
    "    rep = demoji.findall(emoji)\n",
    "    re = demoji.replace(emoji)\n",
    "    if any(rep) == False:\n",
    "        return re\n",
    "    else:\n",
    "        for x in rep:\n",
    "            text = re + rep[x]\n",
    "            return text.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df.apply(lambda x: remove_emoji(x['Tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizando as frases.\n",
    "df['Tokens'] = df.apply(lambda x: word_tokenize(x['Tweet'], language='portuguese'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo pontuação\n",
    "pontos = list(punctuation)\n",
    "\n",
    "def remove_pont(tweets):\n",
    "    return(x for x in tweets if not x in pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokens'] = df.apply(lambda x: remove_pont(x['Tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizando os tweets.\n",
    "def lemmatize_func(mylist):\n",
    "    return [lemmatizer.lemmatize(w) for w in mylist]\n",
    "\n",
    "df['Tokens'] = df.apply(lambda row: lemmatize_func(row['Tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_func(mylist):\n",
    "    return [stemming.stem(w) for w in mylist]\n",
    "\n",
    "df['Tokens'] = df.apply(lambda row: stemming_func(row['Tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec com Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports das bibliotecas\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[lul, livr]                                                                                                                           52\n",
       "[lul, sai, sext]                                                                                                                      24\n",
       "[chor, livr, lul]                                                                                                                     20\n",
       "[lul, tá, livr, babac]                                                                                                                18\n",
       "[lul, tá, solt, babac]                                                                                                                13\n",
       "[lul, sai, sext, nad]                                                                                                                 10\n",
       "[sext, lul, livr]                                                                                                                      7\n",
       "[lul]                                                                                                                                  7\n",
       "[lul, livr, caralh]                                                                                                                    7\n",
       "[lul, livr, porr]                                                                                                                      7\n",
       "[após, solt, lul, bolsonar, cancel, entrev, goiân]                                                                                     6\n",
       "[jnflesch, lul, livr]                                                                                                                  6\n",
       "[chor, livr, lul, lulalivr]                                                                                                            5\n",
       "[titimull, lul, livr]                                                                                                                  5\n",
       "[lul, ladr, roub, coraçã]                                                                                                              5\n",
       "[lul, tá, livr]                                                                                                                        5\n",
       "[lul, livreeeee]                                                                                                                       5\n",
       "[prefeit, paril, comem, solt, lul, faz, convit, ``, esper, ele, '']                                                                    5\n",
       "[lul, deix, carcer, pf, curitib]                                                                                                       4\n",
       "[lul, beij, hoj]                                                                                                                       4\n",
       "[lul, livr, red, heart]                                                                                                                4\n",
       "[lul, livreeeeeee]                                                                                                                     4\n",
       "[urgent, janaín, pascho, exig, impeachment, ministr, stf, lul, ..., via, youtub]                                                       4\n",
       "[agor, sim, curt, qu, beij, sair, pra, beb, comemor, solt, lul, jant, fasc, porr, vc, nunc, irá, sab, thumb, up, light, skin, ton]     4\n",
       "[berni, sand, ``, lul, fez, ninguém, reduz, pobr, brasil, '']                                                                          4\n",
       "[bolsonar, cancel, entrev, cole, após, libert, lul]                                                                                    4\n",
       "[lul, sai, pris, apresent, namor, sel, liberdad, beij, cartacapit]                                                                     4\n",
       "[lul, livreeeeeeeee]                                                                                                                   4\n",
       "[lul, livr, bar, bh, oferec, caip, graç, comemor, liberdad, ex-presid]                                                                 4\n",
       "[marad, comem, lul, pris, ``, hoj, fez, justiç, '']                                                                                    4\n",
       "                                                                                                                                      ..\n",
       "[aguard, ansios, baianasyst, anunci, show, comemor, lul, livr]                                                                         1\n",
       "[hoj, tô, empolg, car, un, amig, pra, dar, rol, ench, car, fal, bem, lul]                                                              1\n",
       "[quer, i, po, loll, c, camis, lul]                                                                                                     1\n",
       "[lul, vai, carr, lembr, jk, gent, louc]                                                                                                1\n",
       "[delucc, lul, profess, milagr]                                                                                                         1\n",
       "[alg, bom, acontec, anoo, etern, presid, lul, livreeeeee]                                                                              1\n",
       "[lul, solt, vei]                                                                                                                       1\n",
       "[emoc, tá, tão, abal, chor, quarent, minut, motiv, lul]                                                                                1\n",
       "[pra, ach, glob, lad, esquerd, engan, bat, lul, jeit, dá, glob, new]                                                                   1\n",
       "[cheg, cas, pai, feliza, fal, q, lul, ta, livr, pau, ta, com, grup, famíl, q, tá, gost]                                                1\n",
       "[lil, cant, sext, lul, livr, caralh]                                                                                                   1\n",
       "[chor, livr, lul, tbm, lulalivrered, heart]                                                                                            1\n",
       "[and, carruag, vão, prend, mor, torn, lul, fich, limp, pergunt, ond, cab, sold]                                                        1\n",
       "[berniesand, bern, mass, vc, lul, aind, vão, vir, aqu, troc, ide, junt, vc, car, man]                                                  1\n",
       "[ate, lul, beij]                                                                                                                       1\n",
       "[lul, saiu, beij]                                                                                                                      1\n",
       "[lul, mal, solt, jair, ex]                                                                                                             1\n",
       "[vej, revolt, liberdad, lul, cert, lad, cert, lulalivr, flag, brazil]                                                                  1\n",
       "[lul, solt, dá, nad, pra, mim, kkkkkkkkk, gal, fel, kkkkkkk, maluc, lul, faz, churrasc, cust, ..b, idiot, kkkkkk]                      1\n",
       "[lul, ladr, roub, coraçã, lil, cant, lulalivrered, heart]                                                                              1\n",
       "[vou, silenci, palavr, lul, bolsonar, n, aguent, ler, mesm, cois]                                                                      1\n",
       "[lul, solt, agor, falt, comeback, rihann]                                                                                              1\n",
       "[aind, algum, otarios/, pobr, direit, critic, liberdad, lul, otár, lament, burr]                                                       1\n",
       "[tud, der, cert, prim, dia, ofic, cigarr, comum, lul, livr, porr]                                                                      1\n",
       "[assim, pai, fal, nad, sobr, lul, tá, doent, pod]                                                                                      1\n",
       "[tia, q, post, lomotif, lul, inst, kkzany, fac]                                                                                        1\n",
       "[tud, conté, lug, dá, assunt, lul]                                                                                                     1\n",
       "[esper, mot, campanh, apen, clich, ``, só, lul, salv, brasil, '', esper, criat]                                                        1\n",
       "[xicograzi, mariadinalva9, lul, pod, tud, ver, ato, viol, próx, dia, engan]                                                            1\n",
       "[gent, lul, tá, livr, pq, vet, lei, porqu, prov, inoc, continuum, send, ladr, defend, ladr, também]                                    1\n",
       "Name: Tokens, Length: 6403, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-12 15:01:24,035 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-11-12 15:01:24,037 : INFO : collecting all words and their counts\n",
      "2019-11-12 15:01:24,037 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-12 15:01:24,062 : INFO : collected 8225 word types from a corpus of 73055 raw words and 6750 sentences\n",
      "2019-11-12 15:01:24,064 : INFO : Loading a fresh vocabulary\n",
      "2019-11-12 15:01:24,091 : INFO : effective_min_count=2 retains 3616 unique words (43% of original 8225, drops 4609)\n",
      "2019-11-12 15:01:24,092 : INFO : effective_min_count=2 leaves 68446 word corpus (93% of original 73055, drops 4609)\n",
      "2019-11-12 15:01:24,108 : INFO : deleting the raw counts dictionary of 8225 items\n",
      "2019-11-12 15:01:24,110 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-11-12 15:01:24,111 : INFO : downsampling leaves estimated 55245 word corpus (80.7% of prior 68446)\n",
      "2019-11-12 15:01:24,135 : INFO : estimated required memory for 3616 words and 150 dimensions: 6147200 bytes\n",
      "2019-11-12 15:01:24,136 : INFO : resetting layer weights\n",
      "2019-11-12 15:01:24,204 : INFO : training model with 10 workers on 3616 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-11-12 15:01:24,223 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:24,241 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:24,273 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:24,298 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:24,301 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:24,330 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:24,334 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:24,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:24,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:24,353 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:24,353 : INFO : EPOCH - 1 : training on 73055 raw words (55252 effective words) took 0.1s, 394182 effective words/s\n",
      "2019-11-12 15:01:24,379 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:24,387 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:24,439 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:24,444 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:24,446 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:24,502 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:24,506 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:24,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:24,516 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:24,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:24,530 : INFO : EPOCH - 2 : training on 73055 raw words (55201 effective words) took 0.2s, 350871 effective words/s\n",
      "2019-11-12 15:01:24,570 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:24,578 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:24,594 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:24,658 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:24,662 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:24,683 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:24,692 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:24,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:24,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:24,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:24,701 : INFO : EPOCH - 3 : training on 73055 raw words (55393 effective words) took 0.2s, 368574 effective words/s\n",
      "2019-11-12 15:01:24,733 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:24,746 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:24,764 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:24,812 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:24,818 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:24,833 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:24,839 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:24,860 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:24,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:24,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:24,874 : INFO : EPOCH - 4 : training on 73055 raw words (55210 effective words) took 0.1s, 373218 effective words/s\n",
      "2019-11-12 15:01:24,911 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:24,931 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:24,938 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:24,976 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:24,994 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:24,997 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,010 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,043 : INFO : EPOCH - 5 : training on 73055 raw words (55251 effective words) took 0.2s, 349712 effective words/s\n",
      "2019-11-12 15:01:25,044 : INFO : training on a 365275 raw words (276307 effective words) took 0.8s, 329064 effective words/s\n",
      "2019-11-12 15:01:25,045 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-11-12 15:01:25,046 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-12 15:01:25,047 : INFO : training model with 10 workers on 3616 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-11-12 15:01:25,112 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,145 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:25,148 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,152 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,173 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:25,185 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,190 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,224 : INFO : EPOCH - 1 : training on 73055 raw words (55362 effective words) took 0.2s, 345581 effective words/s\n",
      "2019-11-12 15:01:25,277 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,297 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-12 15:01:25,299 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,328 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,341 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:25,356 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,374 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,407 : INFO : EPOCH - 2 : training on 73055 raw words (55335 effective words) took 0.2s, 347001 effective words/s\n",
      "2019-11-12 15:01:25,465 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,475 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:25,488 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,526 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,535 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:25,536 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,566 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,577 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,578 : INFO : EPOCH - 3 : training on 73055 raw words (55324 effective words) took 0.2s, 358615 effective words/s\n",
      "2019-11-12 15:01:25,598 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,618 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:25,655 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,702 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,712 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:25,713 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,726 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,746 : INFO : EPOCH - 4 : training on 73055 raw words (55175 effective words) took 0.2s, 346559 effective words/s\n",
      "2019-11-12 15:01:25,766 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,779 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:25,806 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,845 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,855 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:25,863 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:25,867 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:25,872 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:25,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:25,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:25,896 : INFO : EPOCH - 5 : training on 73055 raw words (55215 effective words) took 0.1s, 406433 effective words/s\n",
      "2019-11-12 15:01:25,922 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:25,934 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:25,946 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:25,971 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:25,994 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:26,018 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:26,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:26,029 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:26,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:26,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:26,061 : INFO : EPOCH - 6 : training on 73055 raw words (55317 effective words) took 0.2s, 354758 effective words/s\n",
      "2019-11-12 15:01:26,082 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:26,087 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:26,114 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:26,131 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:26,157 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:26,173 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:26,194 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:26,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:26,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:26,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:26,211 : INFO : EPOCH - 7 : training on 73055 raw words (55187 effective words) took 0.1s, 397497 effective words/s\n",
      "2019-11-12 15:01:26,242 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:26,254 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:26,276 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:26,302 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:26,321 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:26,346 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:26,348 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:26,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:26,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:26,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:26,373 : INFO : EPOCH - 8 : training on 73055 raw words (55333 effective words) took 0.1s, 390983 effective words/s\n",
      "2019-11-12 15:01:26,404 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-11-12 15:01:26,419 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:26,449 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:26,462 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:26,468 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:26,474 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:26,496 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:26,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:26,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:26,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:26,527 : INFO : EPOCH - 9 : training on 73055 raw words (55266 effective words) took 0.1s, 415341 effective words/s\n",
      "2019-11-12 15:01:26,555 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-12 15:01:26,563 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-11-12 15:01:26,600 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-12 15:01:26,613 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-12 15:01:26,637 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-12 15:01:26,642 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-12 15:01:26,647 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-12 15:01:26,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-12 15:01:26,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-12 15:01:26,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-12 15:01:26,677 : INFO : EPOCH - 10 : training on 73055 raw words (55271 effective words) took 0.1s, 424116 effective words/s\n",
      "2019-11-12 15:01:26,678 : INFO : training on a 730550 raw words (552785 effective words) took 1.6s, 339039 effective words/s\n",
      "2019-11-12 15:01:26,678 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(552785, 730550)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinando o modelo\n",
    "model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marciolimaf8', 0.9392938017845154),\n",
       " ('tbm', 0.9371939897537231),\n",
       " ('barretorec', 0.9338074922561646),\n",
       " ('bolsominiom', 0.9323339462280273),\n",
       " ('ein', 0.9316583871841431),\n",
       " ('bord', 0.9245335459709167),\n",
       " ('abrahamweint', 0.9155640602111816),\n",
       " ('carlux', 0.9146286249160767),\n",
       " ('tmb', 0.9114054441452026),\n",
       " ('amo', 0.910111665725708)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procurando palavras semelhantes\n",
    "w1 = ['livr']\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moh', 0.7590863704681396),\n",
       " ('remédi', 0.755330502986908),\n",
       " ('fest', 0.7552500367164612),\n",
       " ('seri', 0.7528912425041199),\n",
       " ('foguet', 0.7526100277900696)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos ver as 5 palavras mais similares a 'lul'\n",
    "w1 = [\"lul\"]\n",
    "model.wv.most_similar(positive=w1, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boc', 0.9440861940383911),\n",
       " ('feir', 0.9391844868659973),\n",
       " ('noil', 0.932273268699646),\n",
       " ('hj', 0.9317667484283447),\n",
       " ('beij', 0.9312781095504761),\n",
       " ('bebemor', 0.9282745122909546)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos ver as 5 palavras mais similares a 'sext'\n",
    "w1 = [\"sext\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('noil', 0.9658463001251221),\n",
       " ('feir', 0.9630030989646912),\n",
       " ('noiv', 0.9605275988578796),\n",
       " ('bebemor', 0.9588722586631775),\n",
       " ('ate', 0.9584187865257263),\n",
       " ('fas', 0.9567884206771851),\n",
       " ('não', 0.9537703394889832),\n",
       " ('colab', 0.9515069723129272),\n",
       " ('sel', 0.9507938027381897),\n",
       " ('plenum', 0.9492806792259216)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# palavras mais relacionadas \n",
    "w1 = ['lul', 'sai', 'sext']\n",
    "model.wv.most_similar (positive=w1,topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similaridade entre palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70240027"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaridade de duas palavras diferentes\n",
    "model.wv.similarity(w1=\"lul\", w2=\"liv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaridades de duas palavras idênticas\n",
    "model.wv.similarity(w1=\"lul\", w2=\"lul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4332605"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaridade de duas palavras opostas\n",
    "model.wv.similarity(w1=\"lul\", w2=\"bolsonar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
